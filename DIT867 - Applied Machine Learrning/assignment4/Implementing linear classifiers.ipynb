{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "863831e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from scipy.linalg.blas import ddot ,  dscal , daxpy\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3bb577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For local directory\n",
    "import os\n",
    "os.chdir('/Users/andreasnilsson/Desktop/Master DS/5. Applied Machine Learning/A 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b39ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "# Read all the documents.\n",
    "X, Y = read_data('all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1babf421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
    "\n",
    "    \n",
    "class LG(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the Logistic Regression learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        lambda_ = 1 / len(Y)\n",
    "        t = 1\n",
    "\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            lf = 0\n",
    "            for x, y in zip(X, Ye):\n",
    "                \n",
    "                t = t + 1\n",
    "                eta = 1 / ( lambda_ * t ) \n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "                lf += max(0,1-y*score)\n",
    "\n",
    "                self.w = ( 1 -  eta *  lambda_) *  self.w  +  (y / (1 + np.exp(y * (score)) )) * x\n",
    "                \n",
    "            store = lf / X.shape[0] + lambda_*(self.w.dot(self.w))/2\n",
    "            print('Value of Objective Function: ' + str(store))\n",
    "                \n",
    "        return X.dot(self.w) \n",
    "\n",
    "\n",
    "class SVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the Support Vector Classifier learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        lambda_ = 1 / len(Y)\n",
    "        \n",
    "        t = 1\n",
    "\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            for x, y in zip(X, Ye):\n",
    "                \n",
    "                t = t + 1\n",
    "                \n",
    "                eta = 1 / ( lambda_ * t ) \n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score <= 1:\n",
    "                    self.w = ( 1 -  eta *  lambda_) *  self.w       + ( eta * y ) *x           \n",
    "                else:\n",
    "                    self.w = ( 1 -  eta *  lambda_) *  self.w \n",
    "        return X.dot(self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d99c0",
   "metadata": {},
   "source": [
    "## Logistic Regression (inc optional task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b1ec5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of Objective Function: 0.5229719253968691\n",
      "Value of Objective Function: 0.45875975661451723\n",
      "Value of Objective Function: 0.46529796395257167\n",
      "Value of Objective Function: 0.4795624927168698\n",
      "Value of Objective Function: 0.49582357804931276\n",
      "Value of Objective Function: 0.512744459129346\n",
      "Value of Objective Function: 0.5297684396024034\n",
      "Value of Objective Function: 0.5466850808421864\n",
      "Value of Objective Function: 0.5633763284643513\n",
      "Value of Objective Function: 0.579660469248121\n",
      "Value of Objective Function: 0.5955811278243239\n",
      "Value of Objective Function: 0.6111457583059363\n",
      "Value of Objective Function: 0.6263727151412634\n",
      "Value of Objective Function: 0.6412650904304197\n",
      "Value of Objective Function: 0.6558346985205822\n",
      "Value of Objective Function: 0.6700833963635533\n",
      "Value of Objective Function: 0.6840206913796444\n",
      "Value of Objective Function: 0.6976488807155251\n",
      "Value of Objective Function: 0.7109750323400353\n",
      "Value of Objective Function: 0.7240085871975569\n",
      "Training time: 3.00 sec.\n",
      "Accuracy: 0.8212.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(), \n",
    "        LG()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86693bd",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72ad0f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.86 sec.\n",
      "Accuracy: 0.8326.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(), \n",
    "        SVC()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b9668",
   "metadata": {},
   "source": [
    "# Bonus task 1. Making your code more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f0efc",
   "metadata": {},
   "source": [
    "## Support Vector Classifier Faster (a part) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "184921dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVC_faster(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        lambda_ = 1 / len(Y)\n",
    "        t = 1\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            for x, y in zip(X, Ye):\n",
    "                \n",
    "                t +=  1\n",
    "                \n",
    "                eta = 1 / ( lambda_ * t ) \n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score =  ddot(x, self.w)\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score < 1:\n",
    "                    self.w = dscal(( 1 -  eta *  lambda_), self.w)  \n",
    "                    daxpy(x, self.w , a = (eta * y))\n",
    "                    \n",
    "                else:\n",
    "                    self.w = dscal(( 1 -  eta *  lambda_), self.w)\n",
    "        return X.dot(self.w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2974af7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.43 sec.\n",
      "Accuracy: 0.8326.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "        #Perceptron()  \n",
    "        SVC_faster()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397b776",
   "metadata": {},
   "source": [
    "The BLAS operation imporves the speed of the code as we see in the print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce03f9d",
   "metadata": {},
   "source": [
    "## B & C questions on Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bc08876",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### The following part is for the optional task.\n",
    "\n",
    "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
    "### Here are two utility functions that help us carry out some vector\n",
    "### operations that we'll need.\n",
    "\n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "class SparseSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "        \n",
    "                \n",
    "        lambda_ = 1 / len(Y)\n",
    "        t = 1\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            a = 1\n",
    "\n",
    "            for x, y in XY:\n",
    "                \n",
    "                t+=1\n",
    "                eta = 1 / (lambda_*t)\n",
    "                \n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = sparse_dense_dot(x, self.w) * a\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score < 1:\n",
    "                    \n",
    "                    a = (1 - eta * lambda_)* a \n",
    "                    add_sparse_to_dense(x, self.w, eta * y / a)\n",
    "                else:\n",
    "                    a = (1 - eta * lambda_)* a\n",
    "                    \n",
    "            self.w = a * self.w\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a9b5319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.67 sec.\n",
      "Accuracy: 0.8410.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        #SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "        SparseSVC()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e91163",
   "metadata": {},
   "source": [
    "If we run SparseSVC without SelectKBest we see the result abouve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cadb41bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 11.87 sec.\n",
      "Accuracy: 0.8410.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        #SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "        SVC()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf24bd",
   "metadata": {},
   "source": [
    "With the initall SVC without SelectKBest the runing time is significantly longer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
