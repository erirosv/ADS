{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load libraries\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics \n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file using Pandas.\n",
    "alldata = pd.read_csv('sberbank.csv')\n",
    "\n",
    "# Convert the timestamp string to an integer representing the year.\n",
    "def get_year(timestamp):\n",
    "    return int(timestamp[:4])\n",
    "alldata['year'] = alldata.timestamp.apply(get_year)\n",
    "\n",
    "# Select the 9 input columns and the output column.\n",
    "selected_columns = ['price_doc', 'year', 'full_sq', 'life_sq', 'floor', 'num_room', 'kitch_sq', 'full_all']\n",
    "alldata = alldata[selected_columns]\n",
    "alldata = alldata.dropna()\n",
    "\n",
    "# Shuffle.\n",
    "alldata_shuffled = alldata.sample(frac=1.0, random_state=0)\n",
    "\n",
    "# Separate the input and output columns.\n",
    "X = alldata_shuffled.drop('price_doc', axis=1)\n",
    "# For the output, we'll use the log of the sales price.\n",
    "Y = alldata_shuffled['price_doc'].apply(np.log)\n",
    "\n",
    "# Split into training and test sets.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.0045619 , 0.00299764, 0.00299978, 0.00305963, 0.00299835]),\n",
       " 'score_time': array([0.0010035 , 0.00099969, 0.00099945, 0.        , 0.        ]),\n",
       " 'test_score': array([-0.39897319, -0.37113485, -0.38083108, -0.39057156, -0.40475168])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "m1 = DummyRegressor()\n",
    "cross_validate(m1, Xtrain, Ytrain, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [2,4]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [10, 17, 25, 33, 41, 48, 56, 64, 72, 80], 'max_features': ['auto', 'sqrt'], 'max_depth': [2, 4], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}\n"
     ]
    }
   ],
   "source": [
    "# Create the param grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_Model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "rf_Grid = GridSearchCV(estimator = rf_Model, param_grid = param_grid, cv = 3, verbose=2, n_jobs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=4,\n",
       "             param_grid={'max_depth': [2, 4], 'max_features': ['auto', 'sqrt'],\n",
       "                         'min_samples_leaf': [1, 2],\n",
       "                         'min_samples_split': [2, 5],\n",
       "                         'n_estimators': [10, 17, 25, 33, 41, 48, 56, 64, 72,\n",
       "                                          80]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_Grid.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy - : 0.369\n",
      "Test Accuracy - : 0.335\n"
     ]
    }
   ],
   "source": [
    "rf_Grid.best_params_\n",
    "print (f'Train Accuracy - : {rf_Grid.score(Xtrain,Ytrain):.3f}')\n",
    "print (f'Test Accuracy - : {rf_Grid.score(Xtest,Ytest):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description:\n",
    "Gradient Boosting regression (GB) is a model which is used to handle problems like classification and regression respectively. GB uses the model with a forward stage-wise, by doing this the model can allow for optimization of the loss funtion which contains arbritaty differentiable. \n",
    "\n",
    "From the Gradient Boosting Regressor we got a train accuracy of: 0.369, and a test accuracy of 0.335. \n",
    "\n",
    "The hyper parameters used were the following:\n",
    "- learning_rate: the learning rate is compressing the share for each tree. The used learning rates is [ 0.25 , 0.3 , 0.35, 0.4,0.5 ]\n",
    "- max_features: number of features to take in to acount when trying to find the best split of n_features. There are three different methods, but in the test we only used two of them, ['auto', 'sqrt']. \n",
    "    - Auto: takes all of n features as a parameter\n",
    "    - Sqrt: takes the square root of the n parameters aa the parameter\n",
    "    - log2: \n",
    "- max_depth: max depth limits the amount of nodes in the regression tree. Different tested depth [ 1, 2, 3, 4, 5,6,7,8,9,10]\n",
    "- min_samples_split: min samples split is the minimum amount of a sample to be splitted into a node. Tested min sample split as hyperparameters [0.001, 0.5, 10]\n",
    "- min_samples_leaf: min samples leaf is teh minimum amount of a sample to create a leaf node. Tested min sample leaf [0.001, 0.03, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #\"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [ 0.25 , 0.3 , 0.35, 0.4,0.5 ], #0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2 , 0.6 ,0.7 , 0.8 ,0.9\n",
    "    \"min_samples_split\": np.linspace(0.001, 0.5, 10),\n",
    "    #\"n_estimators\": [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)],\n",
    "    \"min_samples_leaf\": np.linspace(0.001, 0.03, 5),\n",
    "    \"max_depth\":[ 1, 2, 3, 4, 5,6,7,8,9,10], #1, 2, 3, 4, ,15,20,30,40,50\n",
    "    \"max_features\":[\"log2\",\"sqrt\",'auto'], #1,2,3, ,5,7,9,11,15,17,21\n",
    "    \"criterion\": [\"friedman_mse\",  \"mse\"],\n",
    "    #\"bootstrap\": [True, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "lr_s = GridSearchCV(estimator = regressor, param_grid = param_grid, cv = 3, verbose=2, n_jobs = 4)\n",
    "regressor.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-94.93969323686758\n",
      "[ 5.45064548e-02  1.14202154e-02 -8.44530666e-05  8.31714263e-03\n",
      "  8.99332453e-02 -1.49600124e-04  4.32154471e-08]\n"
     ]
    }
   ],
   "source": [
    "print(regressor.intercept_)\n",
    "print(regressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>15.598902</td>\n",
       "      <td>15.378263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12766</th>\n",
       "      <td>15.464169</td>\n",
       "      <td>15.296538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22648</th>\n",
       "      <td>15.789592</td>\n",
       "      <td>15.576978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12435</th>\n",
       "      <td>15.761421</td>\n",
       "      <td>15.596663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24685</th>\n",
       "      <td>13.815511</td>\n",
       "      <td>15.287299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20047</th>\n",
       "      <td>16.320628</td>\n",
       "      <td>16.077809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28460</th>\n",
       "      <td>15.788202</td>\n",
       "      <td>15.491603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26269</th>\n",
       "      <td>15.830414</td>\n",
       "      <td>15.543966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24955</th>\n",
       "      <td>14.508658</td>\n",
       "      <td>15.423333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14305</th>\n",
       "      <td>14.914123</td>\n",
       "      <td>15.887954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3352 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Actual  Predicted\n",
       "24987  15.598902  15.378263\n",
       "12766  15.464169  15.296538\n",
       "22648  15.789592  15.576978\n",
       "12435  15.761421  15.596663\n",
       "24685  13.815511  15.287299\n",
       "...          ...        ...\n",
       "20047  16.320628  16.077809\n",
       "28460  15.788202  15.491603\n",
       "26269  15.830414  15.543966\n",
       "24955  14.508658  15.423333\n",
       "14305  14.914123  15.887954\n",
       "\n",
       "[3352 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Actual': Ytest, 'Predicted': y_pred})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.3819502113302777\n",
      "Mean Squared Error: 0.3155890397003741\n",
      "Root Mean Squared Error: 0.5617731211978498\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(Ytest, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(Ytest, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Ytest, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24987    15.378263\n",
       "12766    15.296538\n",
       "22648    15.576978\n",
       "12435    15.596663\n",
       "24685    15.287299\n",
       "           ...    \n",
       "20047    16.077809\n",
       "28460    15.491603\n",
       "26269    15.543966\n",
       "24955    15.423333\n",
       "14305    15.887954\n",
       "Name: Predicted, Length: 3352, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Predicted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(estimator=DecisionTreeRegressor(),\n",
      "             param_grid={'max_depth': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10),\n",
      "                         'max_features': ('auto', 'sqrt', 'log2'),\n",
      "                         'min_samples_split': (2, 4, 6, 8, 10)},\n",
      "             scoring=make_scorer(fbeta_score, beta=2))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def fit_model(X, y):\n",
    "    regressor = DecisionTreeRegressor()\n",
    "    \n",
    "    # test parameters for the depth\n",
    "    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10),\n",
    "              'max_features' : ('auto', 'sqrt', 'log2')\n",
    "              , 'min_samples_split' : (2,4,6,8,10)}\n",
    "    scoring_function = make_scorer(fbeta_score, beta=2)\n",
    "    reg = GridSearchCV(regressor, param_grid=parameters, scoring=scoring_function)\n",
    "    print(reg)\n",
    "    reg.fit(X, y)\n",
    "    return reg.best_estimator_\n",
    "\n",
    "reg = fit_model(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=1, max_features='auto')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy - : 0.161\n",
      "Test Accuracy - : 0.153\n"
     ]
    }
   ],
   "source": [
    "print (f'Train Accuracy - : {reg.score(Xtrain,Ytrain):.3f}')\n",
    "print (f'Test Accuracy - : {reg.score(Xtest,Ytest):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description:\n",
    "Decision Tree Regressor/classification (DTR) models have the structure as a tree. The DTR takes in a data set and splits it down to smaller chunks. This leaves the tree structure with decision nodes and leaf nodes respectivly. A node in this case can have two branches as a minimum, this nodes is a representation of teh tested values. The leaf nodes is representing the the actual decision.\n",
    "\n",
    "From the Decision Tree Regressor we got a train accuracy of: 0.161, and a test accuracy of 0.153.\n",
    "\n",
    "Used hyper parameters:\n",
    "- max_depth: maximum depth of the tree. Here we used a series of different max depths (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "- max_features: max features has three different ways to consider the best split (auto, sqrt, log2). \n",
    "- min_samples_leaf: min samples leaf is teh minimum amount of a sample to create a leaf node. Used range [0.001, 0.03, 5] \n",
    "- random_state: random sate is a random control that controls the randomly permuted split. It choose either max_feature or n_feature, depending which of them are smallest. The best split is when these two para meters are equal to each otehr. We used the the following random states [0, 1, 2, 3, 4, 5] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'max_depth': [1, 2, 3, 4, 5],  #6,7,8,9,10,15,20,30,40,50\n",
    "                  'max_features': [1, 2, 3, 4,5,], #6,7,8,9,10,12,14,16,18,19,21\n",
    "                 'random_state':[0, 1, 2, 3, 4, 5],  #10, 15,20,35,50,80,100,150,180,200],\n",
    "                  #'min_samples_split':[1, 2, 5, 7, 10, 15, 20, 30],\n",
    "                  \"min_samples_leaf\": np.linspace(0.001, 0.03, 5)\n",
    "                 #'criterion':['gini','entropy'],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features='sqrt')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = RandomForestRegressor(n_estimators=100,   \n",
    "                             max_depth=None, \n",
    "                             min_samples_split=2, \n",
    "                             min_samples_leaf=1, \n",
    "                             min_weight_fraction_leaf=0.0, \n",
    "                             max_features='sqrt', max_leaf_nodes=None, \n",
    "                             min_impurity_decrease=0.0, bootstrap=True, \n",
    "                             oob_score=False, n_jobs=None, random_state=None, \n",
    "                             verbose=0, warm_start=False, ccp_alpha=0.0, \n",
    "                             max_samples=None)\n",
    "regr.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy - : 0.891\n",
      "Test Accuracy - : 0.276\n"
     ]
    }
   ],
   "source": [
    "print (f'Train Accuracy - : {regr.score(Xtrain,Ytrain):.3f}')\n",
    "print (f'Test Accuracy - : {regr.score(Xtest,Ytest):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description:\n",
    "Random Forest Regressor (RF) works as an estimator to classify numbers that belongs to the decision tree, which contains sampels from the dataset that was provided. RF uses an averageing technique to make its predictions and accuracy. There are also an over-fitting control. The RF model uses the whole dataset to construct the tree.\n",
    "\n",
    "Used hyper parameters:\n",
    "- max_depth: maximum depth of the tree. Here we used a series of different max depths [1, 2, 3, 4,5,6,7, 8,9,10,15,20,30,40,50]\n",
    "- min_samples_split: min samples split is the minimum amount of a sample to be splitted into a node. Used values [0.001, 0.5, 10]\n",
    "- min_samples_leaf: min samples leaf is teh minimum amount of a sample to create a leaf node. Used hyper parameters [0.001, 0.5, 10]\n",
    "- max_features: max features has three different ways to consider the best split. Used features as hyper parameters [log2,sqrt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://www.saedsayad.com/decision_tree_reg.htm [DecisionTreeRegressor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    'max_depth': [1, 2, 3, 4,5,6,7, 8,9,10,15,20,30,40,50],\n",
    "     \"min_samples_split\": np.linspace(0.001, 0.5, 10),\n",
    "     \"min_samples_leaf\": np.linspace(0.001, 0.5, 10),\n",
    "     #'criterion':['auto','gini','entropy'],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
