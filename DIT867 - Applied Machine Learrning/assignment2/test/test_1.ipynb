{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "646bdd93-78c6-412f-944c-8989e62962b9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Task 1: Working with a dataset with categorical features\n",
    "\n",
    "In Assignment 1, we didn't have to do much preprocessing, because all the features in the two datasets were numerical. (Actually, in the second dataset, we removed all non-numerical features.) In this assignment, we'll instead consider how to deal with non-numerical features.\n",
    "\n",
    "We'll use the famous Adult dataset. This is a binary classification task, where our task is to predict whether an American individual earns more than $50,000 a year, given a number of numerical and categorical features. (The dataset was extracted from a 1994 census database.)\n",
    "\n",
    "### Step 1. Reading the data\n",
    "\n",
    "Please download the two CSV files, the training set and the test set, and save them into your working directory This is the official train/test split defined by the people who created the dataset. It's the same data as in the the public distribution, except that we converted the format into a standard CSV format.\n",
    "\n",
    "1. Write code to read the CSV file, for instance by using Pandas as in Assignment 1. \n",
    "2. Then split the data into an input part X and an output part Y. The output variable, which the classifier will predict, is called target.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00001-2cf7b72d-443a-417e-8533-db3f543bd845",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1643270349352,
    "source_hash": "62172251",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00002-91be1b82-c5f5-413c-9f00-59015b23194c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 217,
    "execution_start": 1643267685262,
    "source_hash": "a996211d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import csv file for Training data\n",
    "train = pd.read_csv('adult_train.csv')\n",
    "\n",
    "# Import csv file for Test data\n",
    "test = pd.read_csv('adult_test.csv')\n",
    "\n",
    "# Shuffle the dataset.\n",
    "train_shuffled = train.sample(frac=1.0, random_state=0)\n",
    "test_shuffled = test.sample(frac=1.0, random_state=0)\n",
    "\n",
    "# Split into input part X and output part Y.\n",
    "Xtrain = train_shuffled.drop('target', axis=1)\n",
    "Ytrain = train_shuffled['target']\n",
    "Xtest = test_shuffled.drop('target', axis=1)\n",
    "Ytest = test_shuffled['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00003-fb845d14-2fb8-4bae-93f9-700138ecff2d",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     119
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 36,
    "execution_start": 1643267687419,
    "source_hash": "5bacfead",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22278</th>\n",
       "      <td>49</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8950</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7838</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  workclass    education  education-num      marital-status  \\\n",
       "22278   49  Local-gov      HS-grad              9  Married-civ-spouse   \n",
       "8950    49    Private      HS-grad              9            Divorced   \n",
       "7838    31    Private  Prof-school             15       Never-married   \n",
       "\n",
       "             occupation   relationship   race     sex  capital-gain  \\\n",
       "22278  Transport-moving        Husband  White    Male             0   \n",
       "8950      Other-service  Not-in-family  Black  Female             0   \n",
       "7838     Prof-specialty  Not-in-family  White    Male             0   \n",
       "\n",
       "       capital-loss  hours-per-week native-country  \n",
       "22278             0              40  United-States  \n",
       "8950              0              40  United-States  \n",
       "7838              0              50  United-States  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-a4351898-58da-4eb2-a102-f1b77a2048bb",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Step 2: Encoding the features as numbers.\n",
    "\n",
    "If you look at the data, you will note that it contains several features with categorical values, such as workclass, education etc. All scikit-learn models work with numerical data internally; this means that the categorical features need to be converted to numbers. The most straightforward way to carry out such a conversion is to use one-hot encoding of the features, also known as dummy variables in statistics. In this approach, we define one new column for each observed value of the feature.\n",
    "\n",
    "Scikit-learn includes a number of tools that can do one-hot encoding of categorical features and we'll see how to use one of them, the DictVectorizer. An alternative approach that is a bit more Pandas-friendly and gives more low-level control is to use the recently introduced ColumnTransformer; if you're interested, you can read an introduction to this approach here. We won't use a ColumnTransformer here because it will make Task 3 in this assignment a bit too annoying to solve.\n",
    "\n",
    "The DictVectorizer is used when we store our features as named attributes in dictionaries. For instance, we could represent one individual from the Adult dataset as follows:\n",
    "\n",
    "    {'age': 44,\n",
    "    'workclass': 'Private',\n",
    "    'education': 'Some-college',\n",
    "    'education-num': 10,\n",
    "    'marital-status': 'Married-civ-spouse',\n",
    "    'occupation': 'Machine-op-inspct',\n",
    "    'relationship': 'Husband',\n",
    "    'race': 'Black',\n",
    "    'sex': 'Male',\n",
    "    'capital-gain': 7688,\n",
    "    'capital-loss': 0,\n",
    "    'hours-per-week': 40,\n",
    "    'native-country': 'United-States'}\n",
    "\n",
    "Pandas includes a utility to convert a DataFrame into a list of dictionaries:\n",
    "\n",
    "    dicts_for_my_training_data = my_training_data.to_dict('records')\n",
    "\n",
    "Then make a DictVectorizer and apply it, writing something like the following:\n",
    "\n",
    "    dv = DictVectorizer()\n",
    "    X_train_encoded = dv.fit_transform(dicts_for_my_training_data)\n",
    "\n",
    "The method fit_transform will first call fit, which as usual is the \"training\" method. For a DictVectorizer, \"training\" consists of building the mapping from categories to column positions. Then, the transform method will be called, which converts the data into a matrix.\n",
    "\n",
    "Now that you have a numerical representation of the data, you can compute a cross-validation accuracy for the training set using one of the classifiers you explored in Programming Assignment 1.\n",
    "\n",
    "To handle the test data, you just call transform, because this time the vectorizer does not need to be \"trained.\" Use this to compute the accuracy on the test set.\n",
    "\n",
    "    X_test_encoded = dv.transform(dicts_for_my_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00005-987592fc-9c63-4f31-b63d-ac1f2638e223",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     251.4375
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1058,
    "execution_start": 1643267731953,
    "source_hash": "4fb6f881",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'age': 49,\n",
       "  'workclass': 'Local-gov',\n",
       "  'education': 'HS-grad',\n",
       "  'education-num': 9,\n",
       "  'marital-status': 'Married-civ-spouse',\n",
       "  'occupation': 'Transport-moving',\n",
       "  'relationship': 'Husband',\n",
       "  'race': 'White',\n",
       "  'sex': 'Male',\n",
       "  'capital-gain': 0,\n",
       "  'capital-loss': 0,\n",
       "  'hours-per-week': 40,\n",
       "  'native-country': 'United-States'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert training data to dict - creates a list of dicts \n",
    "# where each row's (person's) information is gathered in one dict, \n",
    "dicts_Xtrain = Xtrain.to_dict('records')\n",
    "dicts_Xtest = Xtest.to_dict('records')\n",
    "\n",
    "# show first 2 person's informations\n",
    "dicts_Xtrain[:1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00009-1f9758cd-a45d-4cfa-94b5-5a6253bd4b72",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1200,
    "execution_start": 1643267742159,
    "source_hash": "b9306252",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create d DictVectorizer \n",
    "dv = DictVectorizer()\n",
    "\n",
    "# one-hot encode the list with all dicts for both train and test\n",
    "Xtrain_encoded = dv.fit_transform(dicts_Xtrain)\n",
    "Xtest_encoded = dv.transform(dicts_Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-810d574a-35bc-4c20-8fcf-b3152431680f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Compute a cross-validation accuracy for the training set using one of the classifiers we explored in Programming Assignment 1.\n",
    "\n",
    "To handle the test data, you just call transform, because this time the vectorizer does not need to be \"trained.\" Use this to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00007-eb89c2f9-81ca-4b1a-aae3-67a443d41d40",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 494,
    "execution_start": 1643270319501,
    "source_hash": "839a496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Classifier\n",
      "Average Train score: 0.7591904454179904\n",
      "Average Test score: 0.7637737239727289\n"
     ]
    }
   ],
   "source": [
    "# Baseline with dummy classifier\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "# train\n",
    "dummy.fit(Xtrain_encoded, Ytrain)\n",
    "# cross validation on train data\n",
    "dummy_score_train = cross_val_score(dummy, Xtrain_encoded, Ytrain).mean()\n",
    "# accuracy on test data\n",
    "dummy_score_test = accuracy_score(Ytest, dummy.predict(Xtest_encoded))\n",
    "\n",
    "print(\"Dummy Classifier\")\n",
    "print(\"Average Train score:\",dummy_score_train)\n",
    "print(\"Average Test score:\",dummy_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "668b8a01-7757-4d73-827e-e91426a2e0b8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14184,
    "execution_start": 1643270361187,
    "source_hash": "55437046",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier\n",
      "Average Train score: 0.8184639981047166\n",
      "Average Test score: 0.817148823782323\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "dtc = DecisionTreeClassifier() \n",
    "# train\n",
    "dtc.fit(Xtrain_encoded, Ytrain)\n",
    "# cross validation on train data\n",
    "dtc_score_train = cross_val_score(dtc, Xtrain_encoded, Ytrain).mean()\n",
    "# accuracy on test data\n",
    "dtc_score_test = accuracy_score(Ytest, dtc.predict(Xtest_encoded))\n",
    "\n",
    "print(\"Decision Tree Classifier\")\n",
    "print(\"Average Train score:\",dtc_score_train)\n",
    "print(\"Average Test score:\",dtc_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "00008-6f3d79df-7a3f-4e50-8491-391fa2b639f0",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     79
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 247827,
    "execution_start": 1643270381249,
    "source_hash": "e5ee612d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier\n",
      "Average Train score: 0.8457663721885279\n",
      "Average Test score: 0.844419875928997\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1) \n",
    "# train\n",
    "rfc.fit(Xtrain_encoded, Ytrain)\n",
    "# cross validation on train data\n",
    "rfc_score_train = cross_val_score(rfc, Xtrain_encoded, Ytrain).mean()\n",
    "# accuracy on test data\n",
    "rfc_score_test = accuracy_score(Ytest, rfc.predict(Xtest_encoded))\n",
    "\n",
    "print(\"Random Forest Classifier\")\n",
    "print(\"Average Train score:\",rfc_score_train)\n",
    "print(\"Average Test score:\",rfc_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-623d27e2-8954-4eb6-8ec8-ccfab0c18e90",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Step 3. Combining the steps.\n",
    "\n",
    "In the example above, we first transformed the list of dictionaries into a numerical matrix, and then we used this matrix when training the classifier. A separate preprocessing step was carried out for the test set.\n",
    "\n",
    "In machine learning setups, we often use long chains of preprocessing steps. The one-hot encoding is one example, and other such steps might be scaling, feature selection, imputation of missing values, etc. As you can imagine, keeping track of the preprocessing steps can be tedious and error-prone, so it makes sense to handle such preprocessing chains automatically.\n",
    "\n",
    "A Pipeline consists of a sequence of scikit-learn modules. The most convenient way to build a Pipeline is to use the utility function make_pipeline. For instance, to build a pipeline consisting of a vectorization step and then a decision tree classifier, we could write\n",
    "\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    \n",
    "    pipeline = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    DecisionTreeClassifier() )\n",
    "    \n",
    "The Pipeline can be treated as any classifier: we can call fit and predict as usual. Concretely, when we call fit on a Pipeline, it will in turn call fit_transform on all intermediate steps and then fit on the final step. When we call predict, transform will be called on the intermediate steps and then predict on the final step.\n",
    "\n",
    "Build a pipeline that includes the classifier that you selected previously, and make sure that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "00037-81b7f10c-53cf-4315-b059-b1605a05b16a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 19,
    "execution_start": 1643274787072,
    "source_hash": "37093721"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data and converting teh data to dics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "00036-c6a78f84-d00c-43be-967e-d0e33c2124ad",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4045,
    "execution_start": 1643274779394,
    "source_hash": "18d108e2"
   },
   "outputs": [],
   "source": [
    "# import train data\n",
    "train_data = pd.read_csv('adult_train.csv')\n",
    "\n",
    "n_cols = len(train_data.columns)\n",
    "\n",
    "# split X and Y from data\n",
    "Xtrain = train_data.iloc[:, :n_cols-1].to_dict('records')\n",
    "Ytrain = train_data.iloc[:, n_cols-1]\n",
    "\n",
    "# import test data\n",
    "test_data = pd.read_csv('adult_test.csv')\n",
    "# split X and Y from data\n",
    "Xtest = test_data.iloc[:, :n_cols-1].to_dict('records')\n",
    "Ytest = test_data.iloc[:, n_cols-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a DictVectorizer and transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "2dcafe9d-9322-42e1-9952-0b3fd2bda8cc",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2340,
    "execution_start": 1643274866271,
    "source_hash": "e92d4af9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create d DictVectorizer \n",
    "dv = DictVectorizer()\n",
    "\n",
    "# one-hot encode the list with all dicts\n",
    "Xtrain_encoded = dv.fit_transform(Xtrain)\n",
    "Xtest_encoded = dv.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the DecisionTreeClassifier and adding hyperparameters that can be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "00040-e93a42fe-a04e-4bae-a271-ea750ce359da",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1643275318734,
    "source_hash": "a51a550a"
   },
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "# specifying parameters for the models\n",
    "param_grid = {'criterion':['gini', 'entropy'], \n",
    "                     'splitter':['best', 'random'],\n",
    "                     'max_features':['auto', 'sqrt', 'log2'],\n",
    "                     'max_depth': [1,2,3,4,5,6,7,8,9,10] }\n",
    "\n",
    "#'max_depth':[1,2,3,4,5,6,7,8,9,10],\n",
    "#'min_samples_leaf': [1,2,3,4,5],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out different parameters for for GridSearchCV and RandomizedSearchCV for the DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "00041-6920adf7-ff3d-4036-8ba1-7ad0a4d6f93d",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     97.9375
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 73025,
    "execution_start": 1643275321935,
    "source_hash": "7b69809c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 10,\n",
       " 'max_features': 'sqrt',\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV\n",
    "gridsearch = GridSearchCV(dtc, param_grid)\n",
    "gridsearch.fit(Xtrain_encoded, Ytrain)\n",
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best sqrt entropy 10\n"
     ]
    }
   ],
   "source": [
    "# storing the best parameter for GridSearchCV\n",
    "g_splitter = gridsearch.best_params_['splitter']\n",
    "g_max_features = gridsearch.best_params_['max_features']\n",
    "g_criterion = gridsearch.best_params_['criterion']\n",
    "g_max_depth = gridsearch.best_params_['max_depth']\n",
    "# priting the best parameters\n",
    "print(g_splitter, g_max_features, g_criterion, g_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "00046-7781411b-9ba3-4608-9c66-d0ee311481d8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6233,
    "execution_start": 1643275415913,
    "source_hash": "405656f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'splitter': 'best',\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 8,\n",
       " 'criterion': 'gini'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV\n",
    "randomsearch = RandomizedSearchCV(dtc, param_grid, n_iter=10)\n",
    "randomsearch.fit(Xtrain_encoded, Ytrain)\n",
    "randomsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "00048-c455569a-9ace-4323-b1db-33d4a2fc7447",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1643275525303,
    "source_hash": "7a6d820f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best sqrt gini 8\n"
     ]
    }
   ],
   "source": [
    "# storing the best parameter for RandomizedSearchCV\n",
    "r_splitter = randomsearch.best_params_['splitter']\n",
    "r_max_features = randomsearch.best_params_['max_features']\n",
    "r_criterion = randomsearch.best_params_['criterion']\n",
    "r_max_depth = randomsearch.best_params_['max_depth']\n",
    "# priting the best parameters\n",
    "print(r_splitter, r_max_features, r_criterion, r_max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines \n",
    "\n",
    "Baseline pipeline without any hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    StandardScaler(with_mean=False),\n",
    "    SelectKBest(k=100),\n",
    "    DecisionTreeClassifier()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('dictvectorizer', DictVectorizer()),\n",
       "                ('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('selectkbest', SelectKBest(k=100)),\n",
       "                ('decisiontreeclassifier', DecisionTreeClassifier())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8158589767213316"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Ytest, pipeline.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines: GridSearchCV and RandomizedSearchCV\n",
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_id": "00050-03b7880e-2151-443a-9792-4c80f539298f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1643275019761,
    "source_hash": "1440e0e7"
   },
   "outputs": [],
   "source": [
    "# GridSearchCV \n",
    "g_pipeline = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    StandardScaler(with_mean=False),\n",
    "    SelectKBest(k=100),\n",
    "    DecisionTreeClassifier(max_features=g_max_features, splitter=g_splitter,\n",
    "                           criterion=g_criterion, max_depth=g_max_depth)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_id": "00051-020cc542-93a9-4590-b3d5-c45e6d820ec1",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     79
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5052,
    "execution_start": 1643275026184,
    "source_hash": "ccd770ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('dictvectorizer', DictVectorizer()),\n",
       "                ('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('selectkbest', SelectKBest(k=100)),\n",
       "                ('decisiontreeclassifier',\n",
       "                 DecisionTreeClassifier(criterion='entropy', max_depth=10,\n",
       "                                        max_features='sqrt'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_pipeline.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "00052-c3fda54a-a1fc-4771-a7a5-04a225098fcd",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     21
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 892,
    "execution_start": 1643275152755,
    "source_hash": "8d29acef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8251335913027456"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Ytest, g_pipeline.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV \n",
    "r_pipeline = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    StandardScaler(with_mean=False),\n",
    "    SelectKBest(k=100),\n",
    "    DecisionTreeClassifier(max_features=r_max_features,splitter=r_splitter,\n",
    "                           criterion=r_criterion, max_depth=r_max_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('dictvectorizer', DictVectorizer()),\n",
       "                ('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('selectkbest', SelectKBest(k=100)),\n",
       "                ('decisiontreeclassifier',\n",
       "                 DecisionTreeClassifier(max_depth=8, max_features='sqrt'))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_pipeline.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8328112523800749"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Ytest, r_pipeline.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy from the pipelines differs, but not to a huge extent. The normal pipeline without patameter tuning had the lowest accuracy. The GridSearchCV with parameter tuning had a slightly higer accuracy then the standard pipeline. Lastly, the RandomizedSearchCV with hyperparameters have teh higest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "447a20e8-08cf-4592-aa0a-6952538f7c1b",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
