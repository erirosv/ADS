{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Train N-Gram Language Models and answer questions\n",
    "\n",
    "\n",
    "This notebook has 20 points.\n",
    "\n",
    "Here we examine how to build count-based MLE language models.\n",
    "\n",
    "\n",
    "## Part 1.1: Language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ngram:\n",
    "_N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "Archive:  wikitext-2-raw-v1.zip\n",
      "replace wikitext-2-raw/wiki.test.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "# Download a wikipedia dataset:\n",
    "! wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
    "! unzip wikitext-2-raw-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus reader\n",
    "# this includes: sentence segmentation and word tokenization:\n",
    "wikitext2 = PlaintextCorpusReader(\n",
    "    'wikitext-2-raw',\n",
    "    ['wiki.train.raw', 'wiki.valid.raw', 'wiki.test.raw'],\n",
    ")\n",
    "word_tokenizer = wikitext2._word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test split:\n",
    "train = wikitext2.sents('wiki.train.raw')\n",
    "test = wikitext2.sents('wiki.test.raw')\n",
    "\n",
    "# the vocabulary based on the training data:\n",
    "vocab = nltk.lm.Vocabulary([\n",
    "    word\n",
    "    for sent in train\n",
    "    for word in sent\n",
    "], unk_cutoff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build n-grams\n",
    "def build_ngrams(sent, n):\n",
    "    # pad both ends for corner-ngrams:\n",
    "    sent = ['<s>']*(n-1) + sent + ['</s>']*(n-1)\n",
    "    # build the ngrams:\n",
    "    return list(ngrams(sent, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_tokinized:\n",
      "['Minecraft', 'is', 'a', 'sandbox', 'video', 'game', 'developed', 'by', 'Mojang', '.']\n",
      "sample_trigrams\n",
      "[('<s>', '<s>', 'Minecraft'), ('<s>', 'Minecraft', 'is'), ('Minecraft', 'is', 'a'), ('is', 'a', 'sandbox'), ('a', 'sandbox', 'video'), ('sandbox', 'video', 'game'), ('video', 'game', 'developed'), ('game', 'developed', 'by'), ('developed', 'by', 'Mojang'), ('by', 'Mojang', '.'), ('Mojang', '.', '</s>'), ('.', '</s>', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "# run this cell to inspect how it works:\n",
    "sample = \"Minecraft is a sandbox video game developed by Mojang.\"\n",
    "sample_tokinized = word_tokenizer.tokenize(sample)\n",
    "sample_trigrams = build_ngrams(sample_tokinized, n=3)\n",
    "print('sample_tokinized:')\n",
    "print(sample_tokinized)\n",
    "print('sample_trigrams')\n",
    "print(sample_trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the count model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "plain: 100%|██████████| 3/3 [00:18<00:00,  6.22s/it]\n",
      "smoothing: 100%|██████████| 3/3 [00:18<00:00,  6.23s/it]\n",
      "smoothing+interpolation: 100%|██████████| 3/3 [00:18<00:00,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.9 s, sys: 1.27 s, total: 56.1 s\n",
      "Wall time: 56.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compare these two models:\n",
    "models = {\n",
    "    'plain': nltk.lm.MLE, # plain count-based ngrams\n",
    "    'smoothing': nltk.lm.Laplace, # with laplace smoothing\n",
    "    'smoothing+interpolation': nltk.lm.KneserNeyInterpolated, # Modified Kneser & Ney \n",
    "}\n",
    "\n",
    "for lm_name in models:\n",
    "    # build and train the language model:\n",
    "    models[lm_name] = models[lm_name](_N, vocabulary=vocab)\n",
    "\n",
    "    # train on all n-grams (equal or lower order): N, N-1, ..., 1.\n",
    "    for n in tqdm(range(_N, 0, -1), desc=lm_name):\n",
    "        models[lm_name].fit([build_ngrams(sent, n) for sent in train])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqDist({'the': 113161, ',': 99925, '.': 78888, 'of': 56889, 'and': 50605, 'in': 39488, 'to': 39190, 'a': 34269, '=': 29570, '\"': 28309, ...}),\n",
       " <ConditionalFreqDist with 75988 conditions>,\n",
       " <ConditionalFreqDist with 701629 conditions>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understand how fit words:\n",
    "# fit() method builds all kinds of count dictionaries:\n",
    "(\n",
    "    models['plain'].counts[1], # unigrams\n",
    "    models['plain'].counts[2], # bi-grams for conditional count freq (w_{t} | w_{t-1})\n",
    "    models['plain'].counts[3], # tri-grams for conditional count freq (w_{t} | w_{t-2} w_{t-1})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('A', 'large'),\n",
       " FreqDist({'number': 4, 'variety': 3, 'portion': 3, 'team': 1, 'tent': 1, 'oil': 1, 'pyramid': 1, 'camp': 1, 'rear': 1, 'network': 1, ...}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example: \n",
    "# Count( word_3='numer'   | word_1 = 'A', word_2 ='large' ) = 4\n",
    "# Count( word_3='variety' | word_1 = 'A', word_2 ='large' ) = 3\n",
    "# ...\n",
    "list(models['plain'].counts[3].items())[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understand this:\n",
    "models['plain'].counts[3][('A', 'large')]['number'] / sum(models['plain'].counts[3][('A', 'large')].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['plain'].score('number', ('A', 'large'))\n",
    "# more details in chapter 3 equation 3.12.\n",
    "# https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fields',\n",
       " '.',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " 'The',\n",
       " 'Lifelong',\n",
       " 'Learning',\n",
       " 'Society',\n",
       " '.',\n",
       " '<UNK>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use the plain model for random language generation:\n",
    "models['plain'].generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.527499220336294"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect log probabilities:\n",
    "models['plain'].logscore('mind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Minecraft is a sandbox video game developed by Mojang.\"\n",
    "sample_ngrams = [\n",
    "    None,\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=1), # unigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=2), # bigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=3), # trigrams\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plain model:\n",
      "1-gram inf\n",
      "2-gram inf\n",
      "3-gram inf\n",
      "\n",
      "smoothing model:\n",
      "1-gram 5089.599724571091\n",
      "2-gram 4387.971551314072\n",
      "3-gram 13897.075640448404\n",
      "\n",
      "smoothing+interpolation model:\n",
      "1-gram 1087.2745339516982\n",
      "2-gram 334.06919150342225\n",
      "3-gram 334.7464277684467\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    print(f\"{model_name} model:\")\n",
    "    for n in range(1, _N+1):\n",
    "        print(f\"{n}-gram\", models[model_name].perplexity(sample_ngrams[n]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why these models have `<UNK>` token? What is the log-probability of <UNK> in three models? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UNK token is used to handle words that are not in the model's vocabulary. These words are called Out-of-vocabulary words, and it's a common thing in real-world text. The log-probability of UNK in a model can vary, it can be set to a low value which indicates the model is less certain about the word, or it can be set to a higher value which indicates the model is more certain. The log-probability of UNK can also depend on the specific dataset and the number of OOV words present in it.\n",
    "\n",
    "```py\n",
    "import transformers\n",
    "\n",
    "model = transformers.BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_ids = torch.tensor([[model.tokenizer.cls_token_id, model.tokenizer.unk_token_id, model.tokenizer.sep_token_id]]).unsqueeze(0)  \n",
    "\n",
    "log_probs = model(input_ids).logits\n",
    "\n",
    "log_prob_of_unk = log_probs[0, 1].item()\n",
    "\n",
    "print(f\"The log-probability of <UNK> is: {log_prob_of_unk}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why plain count-based MLE model fails to produce perplexities? What are the possible solutions for it? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plain count-based Maximum Likelihood Estimation (MLE) model fails to produce perplexities because it does not take into account the rarity of words in the dataset. This means that the model assigns the same probability to all words, regardless of how often they appear in the dataset. This can lead to a high perplexity score for the model, as it will be less likely to correctly predict rare words.\n",
    "\n",
    "One possible solution for this problem is to use a smoothing technique, such as Laplace smoothing or Kneser-Ney smoothing. These methods adjust the probabilities of words in the model to account for their rarity, making the model more robust and better able to handle rare words. Another solution is to use a neural language model, which can learn to assign different probabilities to different words based on the context in which they appear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Show with an example why Laplace smoothing can produce perplexity for unseen words? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace smoothing is a technique used in language modeling to adjust the probabilities \n",
    "of words in the model to account for their rarity. The idea is that if a word is seen \n",
    "very few times in the training data, its probability will be underestimated. Laplace \n",
    "smoothing addresses this issue by adding a small constant to the numerator of the MLE \n",
    "probability estimates, effectively \"smoothing out\" the probabilities of words.\n",
    "\n",
    "For example, consider a simple unigram language model trained on the following sentence: \n",
    "\"The cat sat on the mat.\" The MLE probability of the word \"mat\" would be calculated as:\n",
    "\n",
    "```py\n",
    "P(\"mat\") = count(\"mat\") / total_words\n",
    "```\n",
    "\n",
    "\n",
    "Laplace smoothing, we would add a small constant \"k\" to the numerator, like this:\n",
    "```py\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "unigrams = ngrams(tokens, 1)\n",
    "counts = dict(Counter(unigrams))\n",
    "\n",
    "k = 1\n",
    "vocab_size = len(counts)\n",
    "for word in counts:\n",
    "    probability = (counts[word] + k) / (len(tokens) + (k*vocab_size))\n",
    "    print(f\"P({word}) = {probability}\")\n",
    "\n",
    "```\n",
    "\n",
    "probability of X in this case would be something like this\n",
    "```py\n",
    "P(\"mat\") = (count(\"mat\") + k) / (total_words + (k * vocab_size))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Why perplexity of bi-grams are lower than unigrams? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- answer here (in English and python)\n",
    "- use models['smoothing'].counts[2] to show how?\n",
    "\n",
    "if we compare the counts of unigrams and bigrams using models['smoothing'].counts[1] and models['smoothing'].counts[2], we can see that the bigram model has more information to work with, which results in a lower perplexity score. This is because the bigram model takes into account the previous word in addition to the current word, allowing it to make more accurate predictions. On the other hand, the unigram model only considers the current word and therefore, it may not be able to accurately predict the next word based on the context. This results in a higher perplexity score for unigram models compared to bigram models.\n",
    "\n",
    "```py\n",
    "unigram_counts = models['smoothing'].counts[1]\n",
    "\n",
    "bigram_counts = models['smoothing'].counts[2]\n",
    "\n",
    "if bigram_counts > unigram_counts:\n",
    "  print(\"Bigram model has more information to work with and therefore it results in a lower perplexity score compared to the unigram model\")\n",
    "else:\n",
    "  print(\"Unigram model has more information to work with and therefore it results in a lower perplexity score compared to the bigram model\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 (Optional): neural language models and perplexity of conditional trigrams\n",
    "\n",
    "There are no points for this part but you might want to complete it if you are interested in this topic. We will discuss it again in the fortcoming courses.\n",
    "\n",
    "The neural network below is based on Bengio et al. (2003). It is trained on moving windows described in chapter 9 figure 9.1 but with trigrams instead of 4-grams.\n",
    "https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
    "\n",
    "You don't need to train the model. However, a stand alone python code is provided in `bengio_lm.py` if you want to try training it on GPU.\n",
    "\n",
    "Read the code below then report the perplexity of the language model on the sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # neural network framework\n",
    "\n",
    "# encoding the tokens:\n",
    "vocab_list = [word for word, freq in vocab.counts.most_common() if freq > 1]\n",
    "word2idx = {word: idx for idx, word in enumerate(['<s>', '</s>', vocab.unk_label]+vocab_list)}\n",
    "idx2word = {idx: word for idx, word in enumerate(['<s>', '</s>', vocab.unk_label]+vocab_list)}\n",
    "\n",
    "def token_encoder(tokens):\n",
    "    if type(tokens) in {list, tuple}:\n",
    "        return [word2idx[token] if token in word2idx else word2idx[vocab.unk_label] for token in tokens]\n",
    "    elif type(tokens) == str:\n",
    "        token = tokens\n",
    "        return word2idx[token] if token in word2idx else word2idx[vocab.unk_label]\n",
    "    print(type(tokens))\n",
    "\n",
    "# moving window language model:\n",
    "# https://jmlr.org/papers/volume3/tmp/bengio03a.pdf\n",
    "class BengioLM(torch.nn.Module):\n",
    "    def __init__(self, context_size=2, dim=50):\n",
    "        super(BengioLM, self).__init__()\n",
    "        # defining the parameters of the model\n",
    "        self.C = torch.nn.Embedding(len(word2idx), dim) # C\n",
    "        self.Hx_d = torch.nn.Linear(context_size*dim, dim) # d, H\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.Wx_Uf_b = torch.nn.Linear((context_size + 1) * dim, len(word2idx)) # b, U, W\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.loss_fn = torch.nn.NLLLoss() # negative-log-likelihood loss\n",
    "    \n",
    "    def forward(self, context, target_idx=None):\n",
    "        # function of the model\n",
    "        batch_size = context.shape[0]\n",
    "        x = self.C(context).view(batch_size,-1)\n",
    "        x = torch.cat([x, self.tanh(self.Hx_d(x))], dim=-1)\n",
    "        logprob = self.logsoftmax(self.Wx_Uf_b(x))\n",
    "        \n",
    "        if target_idx is None:\n",
    "            return logprob\n",
    "        else:\n",
    "            loss = self.loss_fn(logprob, target_idx)\n",
    "            return logprob, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is trained with Stochastic Gradient Descent with 10 epochs (skip this):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = BengioLM()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "sentence_batch_size = 32\n",
    "\n",
    "for e in range(10):\n",
    "    progress_bar = tqdm(range(0, len(train), sentence_batch_size))\n",
    "    for i in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        contexts, targets = zip(*[\n",
    "            (token_encoder(tokens[:-1]), token_encoder(tokens[-1]))\n",
    "            for sent in train[i:i+sentence_batch_size]\n",
    "            for tokens in build_ngrams(sent, 3)\n",
    "        ])\n",
    "\n",
    "        logprob, loss = model.forward(torch.tensor(contexts), torch.tensor(targets))\n",
    "        progress_bar.set_description_str(f\"epoch={e+1},loss={loss.item():.3f}\")\n",
    "        progress_bar.update()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we ran the training code above on GPU and saved it in model.pt.\n",
    "# load the pre-trained language model:\n",
    "device = torch.device('cpu')\n",
    "model = BengioLM()\n",
    "model.load_state_dict(torch.load('model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minecraft tensor(-3.5543, grad_fn=<SelectBackward0>)\n",
      "is tensor(-3.8908, grad_fn=<SelectBackward0>)\n",
      "a tensor(-1.8586, grad_fn=<SelectBackward0>)\n",
      "sandbox tensor(-3.5061, grad_fn=<SelectBackward0>)\n",
      "video tensor(-8.3849, grad_fn=<SelectBackward0>)\n",
      "game tensor(-2.7251, grad_fn=<SelectBackward0>)\n",
      "developed tensor(-5.6833, grad_fn=<SelectBackward0>)\n",
      "by tensor(-2.9111, grad_fn=<SelectBackward0>)\n",
      "Mojang tensor(-2.7338, grad_fn=<SelectBackward0>)\n",
      ". tensor(-3.0074, grad_fn=<SelectBackward0>)\n",
      "</s> tensor(-0.0465, grad_fn=<SelectBackward0>)\n",
      "</s> tensor(-1.6689e-06, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# this is how you can get the conditional log-probabilities of all words in the sentence\n",
    "# P(target | w0, w1):\n",
    "for w0, w1, target in build_ngrams(word_tokenizer.tokenize(sample), n=3):\n",
    "    logprobs = model.forward(torch.tensor([token_encoder([w0,w1])]))\n",
    "    print(target, logprobs[0, token_encoder(target)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code here to report Perplexity of the sample sentence.\n",
    "\n",
    "For more information got to chapter 3, section 3.2.1 and chapter 9, equation 9.12.\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y8/fl6_ft9x6rg89xz77sxhp4xc0000gn/T/ipykernel_68142/1149365001.py:12: RuntimeWarning: divide by zero encountered in divide\n",
      "  perplexity = pow(1/prob_sentence, 1/n)\n"
     ]
    }
   ],
   "source": [
    "# perplexity of a sentence \n",
    "# code here\n",
    "import numpy as np \n",
    "n = len(logprobs)\n",
    "probs = np.exp(logprobs.detach().numpy())\n",
    "\n",
    "# calculate the probability of the sentence\n",
    "prob_sentence = np.prod(probs)\n",
    "print(prob_sentence)\n",
    "\n",
    "# calculate the perplexity of the sentence\n",
    "perplexity = pow(1/prob_sentence, 1/n)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a generate function using pre-trained language model above\n",
    "\n",
    "For hints see `bengio_lm.py`, for example how the training loop is implemented. The generation loop would be very similar to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "def generate(model, start_words, token_encoder, max_len=100):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([token_encoder(start_words)], device=device)\n",
    "        generated_text = start_words\n",
    "\n",
    "        for i in range(max_len):\n",
    "            logprobs = model(input_tensor)\n",
    "            word_idx = torch.argmax(logprobs[0][-1]).item()\n",
    "            generated_text.append(token_encoder.decode([word_idx]))\n",
    "            input_tensor = torch.cat((input_tensor, torch.tensor([[word_idx]], device=device)), dim=1)\n",
    "\n",
    "        return ' '.join(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (v3.10.6:9c7b4bd164, Aug  1 2022, 17:13:48) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
