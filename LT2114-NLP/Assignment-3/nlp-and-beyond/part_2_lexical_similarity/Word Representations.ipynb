{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word Representations and Lexical Similarities\n",
    "\n",
    "This part has 20 points in total.\n",
    "\n",
    "Here we will compare different measures of semantic similarity between words: (1) WordNet depth distance (2) cosine similarity of words using a given GloVe model and (3) Resnet50 image features.\n",
    "\n",
    "For more reading on vector semantics got to Chapter 6, sections 6.4 and 6.8:\n",
    "https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "\n",
    "To learn about Wordnet: https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "For additional Wordnet discussions see Chapter 19: https://web.stanford.edu/~jurafsky/slp3/19.pdf\n",
    "\n",
    "The GloVe word embeddings are described in [this paper](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "Resnet50: Deep Residual Learning for Image Recognition are described in [this paper](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "## Part 2.1: Semantic similarity with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# load word-vector glov\n",
    "import gensim.downloader as gensim_api\n",
    "glove_model = gensim_api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "from itertools import combinations, product\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_words = ['car', 'dog', 'banana', 'delicious', 'baguette', 'jumping', 'hugging', 'election']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Word Representations in English WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ezz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ezz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car synsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n",
      "\tSynset('car.n.01') lemmas: [Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n",
      "\tSynset('car.n.01') hypernyms: [Synset('motor_vehicle.n.01')]\n",
      "\tSynset('car.n.01') hyponyms: [Synset('ambulance.n.01'), Synset('beach_wagon.n.01'), Synset('bus.n.04'), Synset('cab.n.03'), Synset('compact.n.03'), Synset('convertible.n.01'), Synset('coupe.n.01'), Synset('cruiser.n.01'), Synset('electric.n.01'), Synset('gas_guzzler.n.01'), Synset('hardtop.n.01'), Synset('hatchback.n.01'), Synset('horseless_carriage.n.01'), Synset('hot_rod.n.01'), Synset('jeep.n.01'), Synset('limousine.n.01'), Synset('loaner.n.02'), Synset('minicar.n.01'), Synset('minivan.n.01'), Synset('model_t.n.01'), Synset('pace_car.n.01'), Synset('racer.n.02'), Synset('roadster.n.01'), Synset('sedan.n.01'), Synset('sport_utility.n.01'), Synset('sports_car.n.01'), Synset('stanley_steamer.n.01'), Synset('stock_car.n.01'), Synset('subcompact.n.01'), Synset('touring_car.n.01'), Synset('used-car.n.01')]\n",
      "\tSynset('car.n.02') lemmas: [Lemma('car.n.02.car'), Lemma('car.n.02.railcar'), Lemma('car.n.02.railway_car'), Lemma('car.n.02.railroad_car')]\n",
      "\tSynset('car.n.02') hypernyms: [Synset('wheeled_vehicle.n.01')]\n",
      "\tSynset('car.n.02') hyponyms: [Synset('baggage_car.n.01'), Synset('cabin_car.n.01'), Synset('club_car.n.01'), Synset('freight_car.n.01'), Synset('guard's_van.n.01'), Synset('handcar.n.01'), Synset('mail_car.n.01'), Synset('passenger_car.n.01'), Synset('slip_coach.n.01'), Synset('tender.n.04'), Synset('van.n.03')]\n",
      "\tSynset('car.n.03') lemmas: [Lemma('car.n.03.car'), Lemma('car.n.03.gondola')]\n",
      "\tSynset('car.n.03') hypernyms: [Synset('compartment.n.02')]\n",
      "\tSynset('car.n.03') hyponyms: []\n",
      "\tSynset('car.n.04') lemmas: [Lemma('car.n.04.car'), Lemma('car.n.04.elevator_car')]\n",
      "\tSynset('car.n.04') hypernyms: [Synset('compartment.n.02')]\n",
      "\tSynset('car.n.04') hyponyms: []\n",
      "\tSynset('cable_car.n.01') lemmas: [Lemma('cable_car.n.01.cable_car'), Lemma('cable_car.n.01.car')]\n",
      "\tSynset('cable_car.n.01') hypernyms: [Synset('compartment.n.02')]\n",
      "\tSynset('cable_car.n.01') hyponyms: []\n",
      "dog synsets: [Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n",
      "\tSynset('dog.n.01') lemmas: [Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')]\n",
      "\tSynset('dog.n.01') hypernyms: [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "\tSynset('dog.n.01') hyponyms: [Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n",
      "\tSynset('frump.n.01') lemmas: [Lemma('frump.n.01.frump'), Lemma('frump.n.01.dog')]\n",
      "\tSynset('frump.n.01') hypernyms: [Synset('unpleasant_woman.n.01')]\n",
      "\tSynset('frump.n.01') hyponyms: []\n",
      "\tSynset('dog.n.03') lemmas: [Lemma('dog.n.03.dog')]\n",
      "\tSynset('dog.n.03') hypernyms: [Synset('chap.n.01')]\n",
      "\tSynset('dog.n.03') hyponyms: []\n",
      "\tSynset('cad.n.01') lemmas: [Lemma('cad.n.01.cad'), Lemma('cad.n.01.bounder'), Lemma('cad.n.01.blackguard'), Lemma('cad.n.01.dog'), Lemma('cad.n.01.hound'), Lemma('cad.n.01.heel')]\n",
      "\tSynset('cad.n.01') hypernyms: [Synset('villain.n.01')]\n",
      "\tSynset('cad.n.01') hyponyms: [Synset('perisher.n.01')]\n",
      "\tSynset('frank.n.02') lemmas: [Lemma('frank.n.02.frank'), Lemma('frank.n.02.frankfurter'), Lemma('frank.n.02.hotdog'), Lemma('frank.n.02.hot_dog'), Lemma('frank.n.02.dog'), Lemma('frank.n.02.wiener'), Lemma('frank.n.02.wienerwurst'), Lemma('frank.n.02.weenie')]\n",
      "\tSynset('frank.n.02') hypernyms: [Synset('sausage.n.01')]\n",
      "\tSynset('frank.n.02') hyponyms: [Synset('vienna_sausage.n.01')]\n",
      "\tSynset('pawl.n.01') lemmas: [Lemma('pawl.n.01.pawl'), Lemma('pawl.n.01.detent'), Lemma('pawl.n.01.click'), Lemma('pawl.n.01.dog')]\n",
      "\tSynset('pawl.n.01') hypernyms: [Synset('catch.n.06')]\n",
      "\tSynset('pawl.n.01') hyponyms: []\n",
      "\tSynset('andiron.n.01') lemmas: [Lemma('andiron.n.01.andiron'), Lemma('andiron.n.01.firedog'), Lemma('andiron.n.01.dog'), Lemma('andiron.n.01.dog-iron')]\n",
      "\tSynset('andiron.n.01') hypernyms: [Synset('support.n.10')]\n",
      "\tSynset('andiron.n.01') hyponyms: []\n",
      "\tSynset('chase.v.01') lemmas: [Lemma('chase.v.01.chase'), Lemma('chase.v.01.chase_after'), Lemma('chase.v.01.trail'), Lemma('chase.v.01.tail'), Lemma('chase.v.01.tag'), Lemma('chase.v.01.give_chase'), Lemma('chase.v.01.dog'), Lemma('chase.v.01.go_after'), Lemma('chase.v.01.track')]\n",
      "\tSynset('chase.v.01') hypernyms: [Synset('pursue.v.02')]\n",
      "\tSynset('chase.v.01') hyponyms: [Synset('hound.v.01'), Synset('quest.v.02'), Synset('run_down.v.07'), Synset('tree.v.03')]\n",
      "banana synsets: [Synset('banana.n.01'), Synset('banana.n.02')]\n",
      "\tSynset('banana.n.01') lemmas: [Lemma('banana.n.01.banana'), Lemma('banana.n.01.banana_tree')]\n",
      "\tSynset('banana.n.01') hypernyms: [Synset('herb.n.01')]\n",
      "\tSynset('banana.n.01') hyponyms: [Synset('abaca.n.02'), Synset('dwarf_banana.n.01'), Synset('edible_banana.n.01'), Synset('japanese_banana.n.01'), Synset('plantain.n.02')]\n",
      "\tSynset('banana.n.02') lemmas: [Lemma('banana.n.02.banana')]\n",
      "\tSynset('banana.n.02') hypernyms: [Synset('edible_fruit.n.01')]\n",
      "\tSynset('banana.n.02') hyponyms: []\n",
      "delicious synsets: [Synset('delicious.n.01'), Synset('delightful.s.01'), Synset('delectable.s.01')]\n",
      "\tSynset('delicious.n.01') lemmas: [Lemma('delicious.n.01.Delicious')]\n",
      "\tSynset('delicious.n.01') hypernyms: [Synset('eating_apple.n.01')]\n",
      "\tSynset('delicious.n.01') hyponyms: [Synset('golden_delicious.n.01'), Synset('red_delicious.n.01')]\n",
      "\tSynset('delightful.s.01') lemmas: [Lemma('delightful.s.01.delightful'), Lemma('delightful.s.01.delicious')]\n",
      "\tSynset('delightful.s.01') hypernyms: []\n",
      "\tSynset('delightful.s.01') hyponyms: []\n",
      "\tSynset('delectable.s.01') lemmas: [Lemma('delectable.s.01.delectable'), Lemma('delectable.s.01.delicious'), Lemma('delectable.s.01.luscious'), Lemma('delectable.s.01.pleasant-tasting'), Lemma('delectable.s.01.scrumptious'), Lemma('delectable.s.01.toothsome'), Lemma('delectable.s.01.yummy')]\n",
      "\tSynset('delectable.s.01') hypernyms: []\n",
      "\tSynset('delectable.s.01') hyponyms: []\n",
      "baguette synsets: [Synset('baguet.n.01')]\n",
      "\tSynset('baguet.n.01') lemmas: [Lemma('baguet.n.01.baguet'), Lemma('baguet.n.01.baguette')]\n",
      "\tSynset('baguet.n.01') hypernyms: [Synset('french_bread.n.01')]\n",
      "\tSynset('baguet.n.01') hyponyms: []\n",
      "jumping synsets: [Synset('jumping.n.01'), Synset('jump.n.06'), Synset('jump.v.01'), Synset('startle.v.02'), Synset('jump.v.03'), Synset('jump.v.04'), Synset('leap_out.v.01'), Synset('jump.v.06'), Synset('rise.v.11'), Synset('jump.v.08'), Synset('derail.v.02'), Synset('chute.v.01'), Synset('jump.v.11'), Synset('jumpstart.v.01'), Synset('jump.v.13'), Synset('leap.v.02'), Synset('alternate.v.01')]\n",
      "\tSynset('jumping.n.01') lemmas: [Lemma('jumping.n.01.jumping')]\n",
      "\tSynset('jumping.n.01') hypernyms: [Synset('track_and_field.n.01')]\n",
      "\tSynset('jumping.n.01') hyponyms: [Synset('broad_jump.n.02'), Synset('high_jump.n.02')]\n",
      "\tSynset('jump.n.06') lemmas: [Lemma('jump.n.06.jump'), Lemma('jump.n.06.jumping')]\n",
      "\tSynset('jump.n.06') hypernyms: [Synset('propulsion.n.02')]\n",
      "\tSynset('jump.n.06') hyponyms: [Synset('capriole.n.01'), Synset('header.n.07'), Synset('hop.n.01'), Synset('jumping_up_and_down.n.01'), Synset('leap.n.01'), Synset('vault.n.04')]\n",
      "\tSynset('jump.v.01') lemmas: [Lemma('jump.v.01.jump'), Lemma('jump.v.01.leap'), Lemma('jump.v.01.bound'), Lemma('jump.v.01.spring')]\n",
      "\tSynset('jump.v.01') hypernyms: [Synset('move.v.03')]\n",
      "\tSynset('jump.v.01') hyponyms: [Synset('bounce.v.01'), Synset('bounce.v.05'), Synset('burst.v.04'), Synset('caper.v.01'), Synset('capriole.v.01'), Synset('curvet.v.01'), Synset('galumph.v.01'), Synset('hop.v.01'), Synset('hop.v.06'), Synset('leapfrog.v.01'), Synset('pronk.v.01'), Synset('saltate.v.02'), Synset('ski_jump.v.01'), Synset('vault.v.01'), Synset('vault.v.02')]\n",
      "\tSynset('startle.v.02') lemmas: [Lemma('startle.v.02.startle'), Lemma('startle.v.02.jump'), Lemma('startle.v.02.start')]\n",
      "\tSynset('startle.v.02') hypernyms: [Synset('move.v.03')]\n",
      "\tSynset('startle.v.02') hyponyms: [Synset('boggle.v.01'), Synset('jackrabbit.v.01'), Synset('rear_back.v.02'), Synset('shy.v.01')]\n",
      "\tSynset('jump.v.03') lemmas: [Lemma('jump.v.03.jump')]\n",
      "\tSynset('jump.v.03') hypernyms: [Synset('assail.v.01')]\n",
      "\tSynset('jump.v.03') hyponyms: []\n",
      "\tSynset('jump.v.04') lemmas: [Lemma('jump.v.04.jump')]\n",
      "\tSynset('jump.v.04') hypernyms: [Synset('wax.v.02')]\n",
      "\tSynset('jump.v.04') hyponyms: []\n",
      "\tSynset('leap_out.v.01') lemmas: [Lemma('leap_out.v.01.leap_out'), Lemma('leap_out.v.01.jump_out'), Lemma('leap_out.v.01.jump'), Lemma('leap_out.v.01.stand_out'), Lemma('leap_out.v.01.stick_out')]\n",
      "\tSynset('leap_out.v.01') hypernyms: [Synset('look.v.02')]\n",
      "\tSynset('leap_out.v.01') hyponyms: []\n",
      "\tSynset('jump.v.06') lemmas: [Lemma('jump.v.06.jump')]\n",
      "\tSynset('jump.v.06') hypernyms: [Synset('enter.v.02')]\n",
      "\tSynset('jump.v.06') hyponyms: []\n",
      "\tSynset('rise.v.11') lemmas: [Lemma('rise.v.11.rise'), Lemma('rise.v.11.jump'), Lemma('rise.v.11.climb_up')]\n",
      "\tSynset('rise.v.11') hypernyms: [Synset('change.v.02')]\n",
      "\tSynset('rise.v.11') hyponyms: []\n",
      "\tSynset('jump.v.08') lemmas: [Lemma('jump.v.08.jump'), Lemma('jump.v.08.leap'), Lemma('jump.v.08.jump_off')]\n",
      "\tSynset('jump.v.08') hypernyms: [Synset('move.v.03')]\n",
      "\tSynset('jump.v.08') hyponyms: []\n",
      "\tSynset('derail.v.02') lemmas: [Lemma('derail.v.02.derail'), Lemma('derail.v.02.jump')]\n",
      "\tSynset('derail.v.02') hypernyms: [Synset('travel.v.01')]\n",
      "\tSynset('derail.v.02') hyponyms: []\n",
      "\tSynset('chute.v.01') lemmas: [Lemma('chute.v.01.chute'), Lemma('chute.v.01.parachute'), Lemma('chute.v.01.jump')]\n",
      "\tSynset('chute.v.01') hypernyms: [Synset('dive.v.01')]\n",
      "\tSynset('chute.v.01') hyponyms: [Synset('sky_dive.v.01')]\n",
      "\tSynset('jump.v.11') lemmas: [Lemma('jump.v.11.jump'), Lemma('jump.v.11.leap')]\n",
      "\tSynset('jump.v.11') hypernyms: []\n",
      "\tSynset('jump.v.11') hyponyms: []\n",
      "\tSynset('jumpstart.v.01') lemmas: [Lemma('jumpstart.v.01.jumpstart'), Lemma('jumpstart.v.01.jump-start'), Lemma('jumpstart.v.01.jump')]\n",
      "\tSynset('jumpstart.v.01') hypernyms: [Synset('start.v.08')]\n",
      "\tSynset('jumpstart.v.01') hyponyms: []\n",
      "\tSynset('jump.v.13') lemmas: [Lemma('jump.v.13.jump'), Lemma('jump.v.13.pass_over'), Lemma('jump.v.13.skip'), Lemma('jump.v.13.skip_over')]\n",
      "\tSynset('jump.v.13') hypernyms: [Synset('neglect.v.01')]\n",
      "\tSynset('jump.v.13') hyponyms: []\n",
      "\tSynset('leap.v.02') lemmas: [Lemma('leap.v.02.leap'), Lemma('leap.v.02.jump')]\n",
      "\tSynset('leap.v.02') hypernyms: [Synset('switch.v.03')]\n",
      "\tSynset('leap.v.02') hyponyms: []\n",
      "\tSynset('alternate.v.01') lemmas: [Lemma('alternate.v.01.alternate'), Lemma('alternate.v.01.jump')]\n",
      "\tSynset('alternate.v.01') hypernyms: [Synset('change.v.03')]\n",
      "\tSynset('alternate.v.01') hyponyms: []\n",
      "hugging synsets: [Synset('caressing.n.01'), Synset('embrace.v.02'), Synset('hug.v.02')]\n",
      "\tSynset('caressing.n.01') lemmas: [Lemma('caressing.n.01.caressing'), Lemma('caressing.n.01.cuddling'), Lemma('caressing.n.01.fondling'), Lemma('caressing.n.01.hugging'), Lemma('caressing.n.01.kissing'), Lemma('caressing.n.01.necking'), Lemma('caressing.n.01.petting'), Lemma('caressing.n.01.smooching'), Lemma('caressing.n.01.snuggling')]\n",
      "\tSynset('caressing.n.01') hypernyms: [Synset('foreplay.n.01')]\n",
      "\tSynset('caressing.n.01') hyponyms: [Synset('snogging.n.01')]\n",
      "\tSynset('embrace.v.02') lemmas: [Lemma('embrace.v.02.embrace'), Lemma('embrace.v.02.hug'), Lemma('embrace.v.02.bosom'), Lemma('embrace.v.02.squeeze')]\n",
      "\tSynset('embrace.v.02') hypernyms: [Synset('clasp.v.01')]\n",
      "\tSynset('embrace.v.02') hyponyms: [Synset('clinch.v.04'), Synset('cuddle.v.02'), Synset('interlock.v.03')]\n",
      "\tSynset('hug.v.02') lemmas: [Lemma('hug.v.02.hug')]\n",
      "\tSynset('hug.v.02') hypernyms: [Synset('touch.v.05')]\n",
      "\tSynset('hug.v.02') hyponyms: []\n",
      "election synsets: [Synset('election.n.01'), Synset('election.n.02'), Synset('election.n.03'), Synset('election.n.04')]\n",
      "\tSynset('election.n.01') lemmas: [Lemma('election.n.01.election')]\n",
      "\tSynset('election.n.01') hypernyms: [Synset('vote.n.02')]\n",
      "\tSynset('election.n.01') hyponyms: [Synset('by-election.n.01'), Synset('general_election.n.01'), Synset('primary.n.01'), Synset('reelection.n.01'), Synset('runoff.n.02')]\n",
      "\tSynset('election.n.02') lemmas: [Lemma('election.n.02.election')]\n",
      "\tSynset('election.n.02') hypernyms: [Synset('choice.n.02')]\n",
      "\tSynset('election.n.02') hyponyms: [Synset('co-option.n.01'), Synset('cumulative_vote.n.01')]\n",
      "\tSynset('election.n.03') lemmas: [Lemma('election.n.03.election')]\n",
      "\tSynset('election.n.03') hypernyms: [Synset('status.n.01')]\n",
      "\tSynset('election.n.03') hyponyms: []\n",
      "\tSynset('election.n.04') lemmas: [Lemma('election.n.04.election')]\n",
      "\tSynset('election.n.04') hypernyms: [Synset('predestination.n.02')]\n",
      "\tSynset('election.n.04') hyponyms: []\n"
     ]
    }
   ],
   "source": [
    "# For each word above print their synsets\n",
    "# for each synset print all lemmas, hypernyms, hyponyms\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "for word in some_words:\n",
    "    synsets = wn.synsets(word)\n",
    "    print(f\"{word} synsets: {synsets}\")\n",
    "    for synset in synsets:\n",
    "        # for each synset print all lemmas\n",
    "        lemmas = synset.lemmas()\n",
    "        print(f\"\\t{synset} lemmas: {lemmas}\")\n",
    "        # for each synset print all hypernyms\n",
    "        hypernyms = synset.hypernyms()\n",
    "        print(f\"\\t{synset} hypernyms: {hypernyms}\")\n",
    "        # for each synset print all hyponyms\n",
    "        hyponyms = synset.hyponyms()\n",
    "        print(f\"\\t{synset} hyponyms: {hyponyms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure The Lexical Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car       dog        0.667\n",
      "car       banana     0.421\n",
      "car       delicious  0.364\n",
      "car       baguette   0.211\n",
      "car       jumping    0.167\n",
      "car       hugging    0.235\n",
      "car       election   0.133\n",
      "dog       banana     0.632\n",
      "dog       delicious  0.556\n",
      "dog       baguette   0.556\n",
      "dog       jumping    0.333\n",
      "dog       hugging    0.286\n",
      "dog       election   0.182\n",
      "banana    delicious  0.750\n",
      "banana    baguette   0.556\n",
      "banana    jumping    0.167\n",
      "banana    hugging    0.250\n",
      "banana    election   0.143\n",
      "delicious baguette   0.500\n",
      "delicious jumping    0.500\n",
      "delicious hugging    0.400\n",
      "delicious election   0.222\n",
      "baguette  jumping    0.154\n",
      "baguette  hugging    0.222\n",
      "baguette  election   0.125\n",
      "jumping   hugging    0.400\n",
      "jumping   election   0.667\n",
      "hugging   election   0.200\n"
     ]
    }
   ],
   "source": [
    "# Wu-Palmer Similarity is a measure of similarity between to sense based on their depth distance. \n",
    "#\n",
    "# For each pair of words, find their closes sense based on Wu-Palmer Similarity.\n",
    "# List all word pairs and their highest possible wup_similarity. \n",
    "# Use wn.wup_similarity(s1, s2) and itertools (combinations and product).\n",
    "# if there is no connection between two words, put 0.\n",
    "\n",
    "wn_sims = []\n",
    "for word1, word2 in combinations(some_words, 2):\n",
    "    max_sim = 0\n",
    "    max_sim = 0\n",
    "    for s1 in wn.synsets(word1):\n",
    "        for s2 in wn.synsets(word2):\n",
    "            sim = wn.wup_similarity(s1, s2)\n",
    "            if sim and sim > max_sim:\n",
    "                max_sim = sim\n",
    "    wn_sims.append(max_sim)\n",
    "    print(f\"{word1:9} {word2:9} {max_sim:6.3f}\")\n",
    "\n",
    "# which word pair are the most similar words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2: Semantic similarity with GloVe and comparison with WordNet\n",
    "\n",
    "### Measure the similarities on GloVe Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car       dog        0.464\n",
      "car       banana     0.219\n",
      "car       delicious  0.068\n",
      "car       baguette   0.046\n",
      "car       jumping    0.516\n",
      "car       hugging    0.278\n",
      "car       election   0.333\n",
      "dog       banana     0.333\n",
      "dog       delicious  0.404\n",
      "dog       baguette   0.018\n",
      "dog       jumping    0.539\n",
      "dog       hugging    0.410\n",
      "dog       election   0.181\n",
      "banana    delicious  0.487\n",
      "banana    baguette   0.450\n",
      "banana    jumping    0.108\n",
      "banana    hugging    0.127\n",
      "banana    election   0.164\n",
      "delicious baguette   0.421\n",
      "delicious jumping    0.042\n",
      "delicious hugging    0.142\n",
      "delicious election   0.028\n",
      "baguette  jumping   -0.075\n",
      "baguette  hugging    0.161\n",
      "baguette  election  -0.091\n",
      "jumping   hugging    0.447\n",
      "jumping   election   0.206\n",
      "hugging   election  -0.076\n"
     ]
    }
   ],
   "source": [
    "glov_sims = []\n",
    "for word1, word2 in combinations(some_words, 2):\n",
    "    max_sim = glove_model.similarity(word1, word2)\n",
    "    glov_sims.append(max_sim)\n",
    "    print(f\"{word1:9} {word2:9} {max_sim:6.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine if two measures correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rho SpearmanrResult(correlation=0.4222499442309076, pvalue=0.02519986065189366)\n"
     ]
    }
   ],
   "source": [
    "# a correlation coefficent of two lists\n",
    "print(\"Spearman's rho\", spearmanr(glov_sims, wn_sims))\n",
    "\n",
    "# Higher correlation (closer to 1.0) means two measures agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the two similarities compare? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation is a value that represents the relationship between two variables. \n",
      "It ranges from -1 to 1. A correlation of 1 means that as one variable increases, \n",
      "the other variable increases as well, indicating a perfect positive relationship. \n",
      "A correlation of -1 means that as one variable increases, the other variable \n",
      "decreases, indicating a perfect negative relationship. A correlation of 0 means\n",
      "that there is no relationship between the variables.\n",
      "\n",
      " In this case the corrrelation is: 0.4222499442309076\n",
      "\n",
      "The P-value is a statistical measure that helps to determine the strength of a \n",
      "correlation. It expresses the probability that the correlation observed between \n",
      "two variables is random. A P-value below 0.05 suggests that the correlation is \n",
      "statistically significant and likely not a result of chance. Conversely, a \n",
      "P-value above 0.05 suggests that the correlation is not statistically significant \n",
      "and may be a result of chance.\n",
      "\n",
      "In this case the pvalues is: 0.02519986065189366\n",
      " \n",
      "As we can see are the pvalue below 0.02, hence there is a statistical significant\n",
      "corrrerlation. Furthermore, The correlation in it self is in the positive end \n",
      "in the range of [-1,1]. Therefore, will most variables chnage when one of them \n",
      "are changes.\n",
      "\n",
      "Correlation tells us how the big chance there are for a variable to be influenced\n",
      "by another. Pvalue tells us if the correlation is significat or not.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correlation, pvalue = spearmanr(glov_sims, wn_sims)\n",
    "corr = \"\"\"\n",
    "Correlation is a value that represents the relationship between two variables. \n",
    "It ranges from -1 to 1. A correlation of 1 means that as one variable increases, \n",
    "the other variable increases as well, indicating a perfect positive relationship. \n",
    "A correlation of -1 means that as one variable increases, the other variable \n",
    "decreases, indicating a perfect negative relationship. A correlation of 0 means\n",
    "that there is no relationship between the variables.\n",
    "\"\"\"\n",
    "\n",
    "pv = \"\"\"\n",
    "The P-value is a statistical measure that helps to determine the strength of a \n",
    "correlation. It expresses the probability that the correlation observed between \n",
    "two variables is random. A P-value below 0.05 suggests that the correlation is \n",
    "statistically significant and likely not a result of chance. Conversely, a \n",
    "P-value above 0.05 suggests that the correlation is not statistically significant \n",
    "and may be a result of chance.\n",
    "\"\"\"\n",
    "\n",
    "tt = \"\"\" \n",
    "As we can see are the pvalue below 0.02, hence there is a statistical significant\n",
    "corrrerlation. Furthermore, The correlation in it self is in the positive end \n",
    "in the range of [-1,1]. Therefore, will most variables chnage when one of them \n",
    "are changes.\n",
    "\n",
    "Correlation tells us how the big chance there are for a variable to be influenced\n",
    "by another. Pvalue tells us if the correlation is significat or not.\n",
    "\"\"\"\n",
    "\n",
    "print(f'{corr}\\n In this case the corrrelation is: {correlation}')\n",
    "print(f'{pv}\\nIn this case the pvalues is: {pvalue}')\n",
    "print(f'{tt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector Representations in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog = [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Each word is represented as a vector:\n",
    "print('dog =', glove_model['dog'])\n",
    "\n",
    "# matrix of all word vectors is trained as parameters of a language model:\n",
    "# P( target_word | context_word ) = f(word, context ; params)\n",
    "#\n",
    "# Words in a same sentence and in close proximity are in context of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Cosine Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6956217"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on equation 6.10 J&M (2019)\n",
    "# https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "#\n",
    "def cosine_sim(v1, v2):\n",
    "    out = 0\n",
    "    dot = np.dot(v1,v2)\n",
    "    norm = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    out = dot / norm\n",
    "    return out\n",
    "\n",
    "cosine_sim(glove_model['car'], glove_model['automobile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement top-n most similar words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 1.0000001), ('cat', 0.9218006), ('dogs', 0.8513159)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search in glove_model:\n",
    "def top_n(word, n):\n",
    "    # example: top_n('dog', 3) =  \n",
    "    #[('cat', 0.9218005537986755),\n",
    "    # ('dogs', 0.8513159155845642),\n",
    "    # ('horse', 0.7907583713531494)]\n",
    "    # similar to glove_model.most_similar('dog', topn=3)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    word_vec = glove_model[word]\n",
    "    for word, vector in zip(glove_model.index_to_key, glove_model.get_normed_vectors()):\n",
    "        sim = cosine_sim(word_vec, vector)\n",
    "        out.append((word, sim))\n",
    "    out.sort(key=lambda x: x[1], reverse=True)\n",
    "    return out[:n]\n",
    "\n",
    "top_n('dog', 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3: Semantic similarity with visual features (ResNet)\n",
    "\n",
    "\n",
    "### Measure the similarities with the ResNet vectors\n",
    "\n",
    "In this part we will use visual features of images representing these objects. If you are interested how we extract these features have a look at `visual-feature-extraction.ipynb` but understanding that notebook is not necessary to complete this part as we have saved them for you they are loaded in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the feature extractor on all images\n",
    "# make sure that the order of features is identical to the order of words (variable some_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car',\n",
       " 'dog',\n",
       " 'banana',\n",
       " 'delicious',\n",
       " 'baguette',\n",
       " 'jumping',\n",
       " 'hugging',\n",
       " 'election']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'car': 0, 'dog': 1, 'banana': 2, 'delicious': 3, 'baguette': 4, 'jumping': 5, 'hugging': 6, 'election': 7}\n"
     ]
    }
   ],
   "source": [
    "object_indices = {v:k for k,v in enumerate(some_words)}\n",
    "print(object_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "image_features = torch.load('image_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2048])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car       dog        0.779\n",
      "car       banana     0.749\n",
      "car       delicious  0.751\n",
      "car       baguette   0.738\n",
      "car       jumping    0.752\n",
      "car       hugging    0.734\n",
      "car       election   0.752\n",
      "dog       banana     0.770\n",
      "dog       delicious  0.786\n",
      "dog       baguette   0.769\n",
      "dog       jumping    0.763\n",
      "dog       hugging    0.803\n",
      "dog       election   0.802\n",
      "banana    delicious  0.791\n",
      "banana    baguette   0.776\n",
      "banana    jumping    0.749\n",
      "banana    hugging    0.751\n",
      "banana    election   0.735\n",
      "delicious baguette   0.778\n",
      "delicious jumping    0.787\n",
      "delicious hugging    0.748\n",
      "delicious election   0.758\n",
      "baguette  jumping    0.750\n",
      "baguette  hugging    0.731\n",
      "baguette  election   0.746\n",
      "jumping   hugging    0.765\n",
      "jumping   election   0.758\n",
      "hugging   election   0.785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "resnet_sims = []\n",
    "\n",
    "# Load the Resnet vectors and create a for loop to compare the words pairwise.\n",
    "\n",
    "# A loop that creates similarities for images pairwise TODO\n",
    "# for resnet\n",
    "for w1, w2 in combinations(some_words, 2):\n",
    "    w1_visfeat = image_features[object_indices[w1]].unsqueeze(0).detach().numpy()\n",
    "    w2_visfeat = image_features[object_indices[w2]].unsqueeze(0).detach().numpy()\n",
    "    max_sim = cosine_similarity(w1_visfeat, w2_visfeat)[0][0]\n",
    "    resnet_sims.append(max_sim)\n",
    "    print(f\"{w1:9} {w2:9} {max_sim:6.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine if Resnet and GloVe similarities correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rho SpearmanrResult(correlation=0.3459222769567597, pvalue=0.07136834743507672)\n"
     ]
    }
   ],
   "source": [
    "# a correlation coefficent of two lists\n",
    "print(\"Spearman's rho\", spearmanr(resnet_sims, glov_sims))\n",
    "\n",
    "# Higher correlation (closer to 1.0) means two measures agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does semantic similarity from word vectors compare with the visual similarity? Are there differences between different words? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In order to compare semantic similarity from word vectors with visual similarity, \n",
      "one could calculate the correlation between the two sets of similarity scores. \n",
      "The correlation coefficient, represented by a value between -1 and 1, measures the \n",
      "strength and direction of the relationship between the two variables. A positive \n",
      "correlation indicates that as one variable increases, the other variable also \n",
      "increases, while a negative correlation indicates that as one variable increases, \n",
      "the other variable decreases. A correlation of 0 indicates no relationship between \n",
      "the variables.\n",
      "\n",
      "\n",
      "The p-value is a statistical measure that is used to determine the significance \n",
      "of the correlation. It represents the probability that the correlation observed \n",
      "between the two sets of similarity scores is due to chance. A low p-value \n",
      "(typically less than 0.05) indicates that the correlation is statistically \n",
      "significant and likely not due to chance, while a high p-value (greater than 0.05) \n",
      "indicates that the correlation is not statistically significant and may be due to chance.\n",
      "\n",
      "\n",
      "It is likely that there will be differences in the correlation and p-value between \n",
      "different words, as the relationship between visual similarity and semantic similarity \n",
      "may vary depending on the specific words being compared. Overall, this will provide a \n",
      "better understanding of how semantic similarity from word vectors compare with the visual \n",
      "similarity and also to know if there are any differences between different words.\n",
      "\n",
      " \n",
      "The semantic similarity from word vectors is a measure of the relatedness of words based \n",
      "on the cosine similarity of their word embeddings, which are vector representations of \n",
      "words learned from large amounts of text data. On the other hand, visual similarity is \n",
      "a measure of how similar two images look to each other.\n",
      "\n",
      "The results of the Spearman's rho correlation show that there is a moderate positive \n",
      "correlation (0.346) between the semantic similarity from word vectors and the visual similarity. \n",
      "However, the p-value (0.071) suggests that this correlation may not be statistically significant.\n",
      "\n",
      "There are differences between different words in terms of the correlation between \n",
      "semantic similarity and visual similarity. Some words may have a strong correlation \n",
      "while others may have a weaker correlation or no correlation at all. It is important \n",
      "to consider the specific words and their context when interpreting the results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your answer here\n",
    "part1 = \"\"\"\n",
    "In order to compare semantic similarity from word vectors with visual similarity, \n",
    "one could calculate the correlation between the two sets of similarity scores. \n",
    "The correlation coefficient, represented by a value between -1 and 1, measures the \n",
    "strength and direction of the relationship between the two variables. A positive \n",
    "correlation indicates that as one variable increases, the other variable also \n",
    "increases, while a negative correlation indicates that as one variable increases, \n",
    "the other variable decreases. A correlation of 0 indicates no relationship between \n",
    "the variables.\n",
    "\"\"\"\n",
    "\n",
    "part2 = \"\"\"\n",
    "The p-value is a statistical measure that is used to determine the significance \n",
    "of the correlation. It represents the probability that the correlation observed \n",
    "between the two sets of similarity scores is due to chance. A low p-value \n",
    "(typically less than 0.05) indicates that the correlation is statistically \n",
    "significant and likely not due to chance, while a high p-value (greater than 0.05) \n",
    "indicates that the correlation is not statistically significant and may be due to chance.\n",
    "\"\"\"\n",
    "\n",
    "part3 = \"\"\"\n",
    "It is likely that there will be differences in the correlation and p-value between \n",
    "different words, as the relationship between visual similarity and semantic similarity \n",
    "may vary depending on the specific words being compared. Overall, this will provide a \n",
    "better understanding of how semantic similarity from word vectors compare with the visual \n",
    "similarity and also to know if there are any differences between different words.\n",
    "\"\"\"\n",
    "\n",
    "comparesing_of_result = \"\"\" \n",
    "The semantic similarity from word vectors is a measure of the relatedness of words based \n",
    "on the cosine similarity of their word embeddings, which are vector representations of \n",
    "words learned from large amounts of text data. On the other hand, visual similarity is \n",
    "a measure of how similar two images look to each other.\n",
    "\n",
    "The results of the Spearman's rho correlation show that there is a moderate positive \n",
    "correlation (0.346) between the semantic similarity from word vectors and the visual similarity. \n",
    "However, the p-value (0.071) suggests that this correlation may not be statistically significant.\n",
    "\n",
    "There are differences between different words in terms of the correlation between \n",
    "semantic similarity and visual similarity. Some words may have a strong correlation \n",
    "while others may have a weaker correlation or no correlation at all. It is important \n",
    "to consider the specific words and their context when interpreting the results.\n",
    "\"\"\"\n",
    "\n",
    "print(part1)\n",
    "print(part2)\n",
    "print(part3)\n",
    "print(comparesing_of_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.4 Optional: Examine Fairness In Data Driven Word Vectors\n",
    "\n",
    "There are no points for this part but you are welcome to further explore this topic if you are inetrested in it. We will address it again in the Computational semantics course.\n",
    "\n",
    "Caliskan et al. (2017) argues that word vectors learn human biases from data. \n",
    "\n",
    "Try to replicate one of the tests of the paper:\n",
    "\n",
    "Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. “Semantics derived automatically from language corpora contain human-like biases.” Science\n",
    "356.6334 (2017): 183-186. http://opus.bath.ac.uk/55288/\n",
    "\n",
    "\n",
    "For example on gender bias:\n",
    "- Male names: John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill.\n",
    "- Female names: Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna.\n",
    "- Career words : executive, management, professional, corporation, salary, office, business, career.\n",
    "- Family words : home, parents, children, family, cousins, marriage, wedding, relatives.\n",
    "\n",
    "\n",
    "Report the average cosine similarity of male names to career words, and compare it with the average similarity of female names to career words. (repeat for family words) \n",
    "\n",
    "tokens in GloVe model are all in lower case.\n",
    "\n",
    "Write at least one sentence to describe your observation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic similarity of words is a way to see how related words are by comparing their word embeddings, which are like map points for words made from analyzing lots of text. Visual similarity is how much two pictures look alike.\n",
    "\n",
    "The Spearman's rho test showed that there is a medium connection (0.346) between semantic similarity and visual similarity. However, the test also showed that this connection may not be very strong because of the low p-value (0.071).\n",
    "\n",
    "Different words can have different levels of connection between semantic similarity and visual similarity. Some words may have a strong connection, while others might not have a connection at all. It's important to consider the specific words and what they mean when looking at these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the word_embeddings tensor: torch.Size([8, 2048])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for dimension 0 with size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     word_to_ix[word] \u001b[39m=\u001b[39m i\n\u001b[1;32m     17\u001b[0m word_indices \u001b[39m=\u001b[39m [word_to_ix[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m male_names \u001b[39m+\u001b[39m female_names \u001b[39m+\u001b[39m career_words \u001b[39m+\u001b[39m family_words]\n\u001b[0;32m---> 18\u001b[0m word_embeddings \u001b[39m=\u001b[39m word_embeddings[word_indices, :]\n\u001b[1;32m     20\u001b[0m num_words \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(male_names) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(female_names) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(career_words) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(family_words)\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m num_words \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(word_embeddings):\n",
      "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for dimension 0 with size 8"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "word_embeddings = torch.load('image_features.pt')\n",
    "\n",
    "print(\"Size of the word_embeddings tensor:\", word_embeddings.shape)\n",
    "\n",
    "male_names = [\"john\", \"paul\", \"mike\", \"kevin\", \"steve\", \"greg\", \"jeff\", \"bill\"]\n",
    "female_names = [\"amy\", \"joan\", \"lisa\", \"sarah\", \"diana\", \"kate\", \"ann\", \"donna\"]\n",
    "career_words = [\"executive\", \"management\", \"professional\", \"corporation\", \"salary\", \"office\", \"business\", \"career\"]\n",
    "family_words = [\"home\", \"parents\", \"children\", \"family\", \"cousins\", \"marriage\", \"wedding\", \"relatives\"]\n",
    "\n",
    "word_to_ix = {}\n",
    "for i, word in enumerate(male_names + female_names + career_words + family_words):\n",
    "    word_to_ix[word] = i\n",
    "\n",
    "word_indices = [word_to_ix[word] for word in male_names + female_names + career_words + family_words]\n",
    "word_embeddings = word_embeddings[word_indices, :]\n",
    "\n",
    "num_words = len(male_names) + len(female_names) + len(career_words) + len(family_words)\n",
    "if num_words != len(word_embeddings):\n",
    "    raise ValueError(\"Number of words and number of embeddings do not match\")\n",
    "\n",
    "male_names_embeddings = word_embeddings[:len(male_names), :]\n",
    "career_words_embeddings = word_embeddings[len(male_names) + len(female_names):, :]\n",
    "\n",
    "male_career_similarities = np.dot(male_names_embeddings, career_words_embeddings.T) / (np.linalg.norm(male_names_embeddings, axis=1, keepdims=True) * np.linalg.norm(career_words_embeddings, axis=1))\n",
    "average_male_career_similarity = male_career_similarities.mean()\n",
    "\n",
    "female_names_embeddings = word_embeddings[len(male_names):len(male_names)+len(female_names), :]\n",
    "\n",
    "female_career_similarities = np.dot(female_names_embeddings, career_words_embeddings.T) / (np.linalg.norm(female_names_embeddings, axis=1, keepdims=True) * np.linalg.norm(career_words_embeddings, axis=1))\n",
    "average_female_career_similarity = female_career_similarities.mean()\n",
    "\n",
    "# Compare the results\n",
    "print(f\"Average similarity of male names to career words: {average_male_career_similarity}\")\n",
    "print(f\"Average similarity of female names to career words: {average_female_career_similarity}\")\n",
    "\n",
    "# Repeat for family words\n",
    "family_word_embeddings = []\n",
    "for word in family_words:\n",
    "    word_embedding = word_embeddings[word_to_ix[word], :]\n",
    "    family_word_embeddings.append(word_embedding)\n",
    "family_word_embeddings = np.array(family_word_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (v3.10.6:9c7b4bd164, Aug  1 2022, 17:13:48) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
